{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "833634a8-c37c-4997-b11b-306e0766edab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ab0e7ed7-2fa9-4177-b702-0f4c85b2f3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "def evaluate_preds(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Preforms evaluation comparison on y_true labels vs y_pred labels on our classification model.\n",
    "    \"\"\"\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    metric_dict = {\"accuracy\" : round(accuracy, 2), \"precision\" : round(precision, 2), \"recall\" : round(recall, 2), \"f1\" : round(f1, 1)}\n",
    "    print(f\"Acc: {accuracy * 100:.2f}%\")\n",
    "    print(f\"Prec: {precision * 100:.2f}%\")\n",
    "    print(f\"Rec: {recall:.2f}%\")\n",
    "    print(f\"f1: {f1:.2f}%\")\n",
    "\n",
    "    return metric_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09666de0-6d66-407b-8d2b-767fd968f4e6",
   "metadata": {},
   "source": [
    "## Saving and Loading trained ML model\n",
    "\n",
    "Two ways to save and load ML models:\n",
    "1. With Pythons `pickle` module\n",
    "2. WIth `joblib` module\n",
    "\n",
    "**Pickle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6df9929d-6468-4784-97ac-0d7d28d40fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.1s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.2s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.2s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.2s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.2s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.2s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV ##SIMILAR TO RANDOMIZED but it will go through all combinations of params\n",
    "\n",
    "grid_2 = {'n_estimators': [100, 200],\n",
    "          'max_depth': [None],\n",
    "          'max_features': ['sqrt'],\n",
    "          'min_samples_split': [2],\n",
    "          'min_samples_leaf': [2, 4]}\n",
    "\n",
    "heart_disease = pd.read_csv(\"heart-disease.csv\")\n",
    "np.random.seed(42)\n",
    "\n",
    "# SUFFLE THE DATA\n",
    "heart_disease_shuffeled = heart_disease.sample(frac=1)\n",
    "\n",
    "X = heart_disease_shuffeled.drop(\"target\", axis=1)\n",
    "y = heart_disease_shuffeled[\"target\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "clf = RandomForestClassifier(n_jobs=1)\n",
    "\n",
    "#Setup Grid search cv\n",
    "gs_clf = GridSearchCV(estimator=clf, \n",
    "                            param_grid=grid_2,\n",
    "                            cv=5,\n",
    "                            verbose=2)\n",
    "\n",
    "#FIt the grid search cv version of clf\n",
    "gs_clf.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "37d3933c-d510-4d42-913d-b713198c04f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save an existing model to file\n",
    "import pickle\n",
    "pickle.dump(gs_clf, open(\"gs_random_forest_model_1.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e1498d38-f4e3-410e-9915-3e465f84dc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import existing model from file\n",
    "loaded_pickle_model = pickle.load(open(\"gs_random_forest_model_1.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "80b13bc0-5fb4-4c39-8614-6e172b4daef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 90.16%\n",
      "Prec: 88.89%\n",
      "Rec: 0.94%\n",
      "f1: 0.91%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9, 'precision': 0.89, 'recall': 0.94, 'f1': 0.9}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make some predictions\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) \n",
    "loaded_y_preds = loaded_pickle_model.predict(X_test)\n",
    "evaluate_preds(y_test, loaded_y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1e6c3ffc-b7ec-4de3-bd43-a58694434c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 90.16%\n",
      "Prec: 88.89%\n",
      "Rec: 0.94%\n",
      "f1: 0.91%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9, 'precision': 0.89, 'recall': 0.94, 'f1': 0.9}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_y_preds = gs_clf.predict(X_test)\n",
    "evaluate_preds(y_test, gs_y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc86784-3b6f-420d-80cf-608d6f876550",
   "metadata": {},
   "source": [
    "**joblib**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "81bf3236-12a6-46b7-aa6e-422777ad2240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gs_random_forest_model_1.joblib']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "\n",
    "#Save model to file\n",
    "dump(gs_clf, filename=\"gs_random_forest_model_1.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "587f42ee-e391-4dfe-a4fa-b5fb3f7ea551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the existing model\n",
    "loaded_joblib_model = load(filename=\"gs_random_forest_model_1.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b4dc037b-63ce-4006-b7e6-5891161f6787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 90.16%\n",
      "Prec: 88.89%\n",
      "Rec: 0.94%\n",
      "f1: 0.91%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9, 'precision': 0.89, 'recall': 0.94, 'f1': 0.9}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make some predictions\n",
    "jl_loaded_y_preds = loaded_joblib_model.predict(X_test)\n",
    "evaluate_preds(y_test, jl_loaded_y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "60153572-8cc8-4b70-b6b1-73e89da18bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 90.16%\n",
      "Prec: 88.89%\n",
      "Rec: 0.94%\n",
      "f1: 0.91%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9, 'precision': 0.89, 'recall': 0.94, 'f1': 0.9}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_y_preds = gs_clf.predict(X_test)\n",
    "evaluate_preds(y_test, gs_y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ced9693-847b-443e-87b0-38ded546856f",
   "metadata": {},
   "source": [
    "**According to sklearn documentation it may be better to use joblib library which is more efficient on object that carry large numpy arrays internally as it is often the case for fitted sklearn estimators, but can only pickle to the disk and not to string**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
